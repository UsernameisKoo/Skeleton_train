{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1xniF2Illj7Hu67ja7Kud7V7KocWgiLAC","authorship_tag":"ABX9TyPtmwRC9i64Xl8SEgYkzL2Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rV-A6VlWSY1x"},"outputs":[],"source":["!nvidia-smi  # GPU 확인\n","\n","# 1. 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 2. 클론할 위치로 이동\n","%cd /content/drive/MyDrive/Colab_Notebooks/Projects/3-2/CV\n","\n","# 3. GitHub 레포 클론\n","!git clone https://github.com/MeshalAlamr/quick-action-recognition.git\n","\n","# 4. 프로젝트 폴더로 이동\n","%cd quick-action-recognition\n","\n","# 5. 라이브러리 설치\n","!pip install torch torchvision\n","!pip install numpy tqdm\n"]},{"cell_type":"code","source":["!pip install gdown\n","\n","FILE_ID = \"103NOL9YYZSW1hLoWmYnv5Fs8mK-Ij7qb\"\n","ZIP_NAME = \"dataset.zip\"\n","\n","!gdown --id {FILE_ID} -O {ZIP_NAME}\n","!mkdir -p data\n","!unzip -q {ZIP_NAME} -d data\n","\n","print(\"다운로드 & 압축 해제 완료!\")\n"],"metadata":{"id":"2u_mJz5xSfZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","path = \"/content/quick-action-recognition/data/data/NTU-RGB-D/xview/\"\n","\n","# Load the data\n","data = np.load(path + \"train_data.npy\")\n","\n","# Shape = (# of videos, coordinate axes, # of frames, # of joints, # of subjects)\n","shape = list(data.shape)\n","\n","# Halve the number of frames\n","shape[2] = int(shape[2]/2)\n","\n","# Initialization\n","downsampled_data = np.zeros(shape,dtype=np.float32)\n","\n","# Downsample the data, take one frame leave another\n","x=0\n","for i in range(300):\n","    if i%2==0:\n","        downsampled_data[:,:,x,:,:] = data[:,:,i,:,:]\n","        x = x+1\n","\n","# Save the downsampled data\n","np.save(path + \"train_data_downsampled.npy\", downsampled_data)"],"metadata":{"id":"chQwj9tmX7lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Val/Test ---\n","val = np.load(path + \"val_data.npy\")\n","val_ds = val[:, :, ::2, :, :]\n","np.save(path + \"val_data_downsampled.npy\", val_ds)"],"metadata":{"id":"ckmV9teNaANE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","\n","# Selected actions (Note: 0 means action \"A1\" and so on)\n","actions = [0, 1, 7, 8, 10, 11, 27, 28, 29, 40, 102, 103]\n","\n","path = \"/content/quick-action-recognition/data/data/NTU-RGB-D/xview/\"\n","# Load training and testing data and labels\n","train_data = np.load(path + \"train_data_downsampled.npy\")\n","test_data = np.load(path + \"val_data_downsampled.npy\")\n","with open(path + \"train_label.pkl\",\"rb\") as f:\n","    train_label = pickle.load(f)\n","with open(path + \"val_label.pkl\",\"rb\") as f:\n","    test_label = pickle.load(f)\n","\n","# Find index for selected actions\n","train_index = [i for i, x in enumerate(train_label[1]) if x in actions]\n","test_index = [i for i, x in enumerate(test_label[1]) if x in actions]\n","\n","# Create & save new training and testing data\n","small_train_data = train_data[train_index]\n","np.save(path + \"small_train_data.npy\", small_train_data)\n","small_test_data = test_data[test_index]\n","np.save(path + \"small_val_data.npy\", small_test_data)\n","\n","# Create & save new training and testing labels\n","small_train_label = np.array(train_label)\n","small_train_label = small_train_label[:,train_index]\n","small_test_label = np.array(test_label)\n","small_test_label = small_test_label[:,test_index]\n","\n","with open(path + \"small_train_label.pkl\", \"wb\") as f:\n","    pickle.dump(small_train_label, f)\n","\n","with open(path + \"small_val_label.pkl\", \"wb\") as f:\n","    pickle.dump(small_test_label, f)"],"metadata":{"id":"SRt7X9KVUHlZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EwD-CcuMbQb_"},"execution_count":null,"outputs":[]}]}