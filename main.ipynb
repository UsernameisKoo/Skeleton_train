{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","TARGET_DIR = \"/content/drive/MyDrive/your_path\"\n","PROJECT_NAME = \"quick-action-recognition\"\n","\n","resultsFolder = f\"{TARGET_DIR}/{PROJECT_NAME}/\"\n","\n","import os\n","os.makedirs(resultsFolder, exist_ok=True)"],"metadata":{"id":"oZhh_Exxinjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"N2KqZNKwii6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NniNWpwriM4I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.optim as optim\n","import pickle\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import WeightedRandomSampler\n","import pandas as pd\n","import random\n","from torch.utils.data import Dataset\n","from torch.amp import autocast, GradScaler\n","from sklearn.metrics import confusion_matrix, classification_report, f1_score\n","from torch.optim.lr_scheduler import OneCycleLR\n","\n","#########################################################\n","torch.cuda.empty_cache()\n","torch.backends.cudnn.benchmark = True\n","\n","batch_size = 16\n","num_epoch = 200\n","lr = 3e-3\n","dropout = 0.2\n","weight_decay = 5e-5\n","experiment = \"stgcn_onecyclelr\"\n","resultsFile = experiment + \".csv\"\n","state_path = resultsFolder + experiment + \".pth\"\n","path = f\"{TARGET_DIR}/{PROJECT_NAME}/data/NTU-RGB-D/x-view/\"\n","\n","resume_training = False  # True로 설정하면 이어서 학습\n","resume_epoch = 46\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","features_train = torch.FloatTensor(np.load(path + \"small_train_data.npy\"))\n","features_val   = torch.FloatTensor(np.load(path + \"small_val_data.npy\"))\n","features_test  = torch.FloatTensor(np.load(path + \"small_test_data.npy\"))\n","\n","mean = features_train.mean(dim=(0, 2, 3, 4), keepdim=True)\n","std  = features_train.std(dim=(0, 2, 3, 4), keepdim=True) + 1e-5\n","\n","features_train = (features_train - mean) / std\n","features_val   = (features_val   - mean) / std\n","features_test  = (features_test  - mean) / std\n","\n","\n","labels_train_all = pickle.load(open(path + \"small_train_label.pkl\", \"rb\"))   # shape (2, N)\n","labels_val_all   = pickle.load(open(path + \"small_val_label.pkl\", \"rb\"))  # shape (2, N)\n","labels_test_all  = pickle.load(open(path + \"small_test_label.pkl\", \"rb\"))    # shape (2, N)\n","\n","labels_train = np.array(labels_train_all[1], dtype=int)\n","labels_val   = np.array(labels_val_all[1], dtype=int)\n","labels_test  = np.array(labels_test_all[1], dtype=int)\n","\n","\n","#########################################################\n","# 클래스 리라벨링: 0 ~ num_class-1 로 통일\n","\n","actions = np.unique(labels_train)  # train에 등장하는 클래스 기준\n","label_map = {k: i for i, k in enumerate(actions)}\n","\n","for old, new in label_map.items():\n","    labels_train[labels_train == old] = new\n","    labels_val[labels_val == old] = new\n","    labels_test[labels_test == old] = new\n","\n","num_class = len(actions)\n","\n","#################### Sampler\n","\n","class_sample_count = np.array([len(np.where(labels_train == t)[0]) for t in np.unique(labels_train)])\n","weight = 1. / class_sample_count\n","samples_weight = np.array([weight[int(t)] for t in labels_train])\n","samples_weight = torch.from_numpy(samples_weight)\n","samples_weight = samples_weight.double()\n","sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n","\n","class_weights = torch.tensor(weight, dtype=torch.float32)\n","class_weights = class_weights / class_weights.mean()\n","class_weights = class_weights.to(device)\n","\n","####################### Data Augmentation\n","\n","def random_augment_skeleton(x, p_shift=0.3, p_scale=0.3, p_noise=0.3):\n","    \"\"\"\n","    x: tensor of shape (C, T, V, M)\n","    \"\"\"\n","    C, T, V, M = x.shape\n","\n","    # 1) 시간축 랜덤 시프트\n","    if random.random() < p_shift:\n","        max_shift = max(1, T // 40)  # 전체 길이의 2.5% 정도까지\n","        shift = random.randint(-max_shift, max_shift)\n","        x = torch.roll(x, shifts=shift, dims=1)  # dim=1: T\n","\n","    # 2) 전체 스켈레톤 스케일링\n","    #if random.random() < p_scale:\n","    #    scale = 1.0 + 0.1 * (2 * random.random() - 1)  # 0.9 ~ 1.1\n","    #    x = x * scale\n","\n","    # 3) 좌표에 가우시안 노이즈 추가\n","    if random.random() < p_noise:\n","        noise = torch.randn_like(x) * 0.005  # sigma=0.005 정도\n","        x = x + noise\n","\n","    return x\n","class SkeletonDataset(Dataset):\n","    def __init__(self, features, labels, augment=False):\n","        \"\"\"\n","        features: tensor (N, C, T, V, M)\n","        labels: 1D tensor (N,)\n","        augment: train일 때만 True\n","        \"\"\"\n","        self.features = features\n","        self.labels = labels\n","        self.augment = augment\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = self.features[idx]          # (C, T, V, M)\n","        y = self.labels[idx]\n","\n","        # train일 때만 augmentation\n","        if self.augment:\n","            x = random_augment_skeleton(x)\n","\n","        return x, y\n","\n","####################### Trainloader and testloader\n","labels_train_t = torch.LongTensor(labels_train)\n","labels_val_t   = torch.LongTensor(labels_val)\n","labels_test_t  = torch.LongTensor(labels_test)\n","\n","# === 커스텀 Dataset 사용 ===\n","train_dataset = SkeletonDataset(features_train, labels_train_t, augment=True)   # ★ train만 augment=True\n","val_dataset   = SkeletonDataset(features_val,   labels_val_t,   augment=False)\n","test_dataset  = SkeletonDataset(features_test,  labels_test_t,  augment=False)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    drop_last=True,\n","    num_workers=2,\n","    persistent_workers=True,\n","    sampler=sampler,                        # sampler는 그대로 사용\n","    pin_memory=(device.type == \"cuda\"),\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    drop_last=False,\n","    num_workers=2,\n","    shuffle=False,\n","    pin_memory=(device.type == \"cuda\"),\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    drop_last=False,\n","    num_workers=2,\n","    shuffle=False,\n","    pin_memory=(device.type == \"cuda\"),\n",")\n","\n","# 메모리 아끼기\n","del features_train, labels_train_t\n","del features_val, labels_val_t\n","del features_test, labels_test_t\n","\n","def get_edge():\n","    num_node = 25\n","    self_link = [(i, i) for i in range(num_node)]\n","    neighbor_1base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n","                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n","                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n","                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n","                      (22, 23), (23, 8), (24, 25), (25, 12)]\n","    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n","    edge = self_link + neighbor_link\n","    center = 21 - 1\n","    return (edge, center)\n","\n","\n","def get_hop_distance(num_node, edge, max_hop=1):\n","    A = np.zeros((num_node, num_node))\n","    for i, j in edge:\n","        A[j, i] = 1\n","        A[i, j] = 1\n","\n","    hop_dis = np.zeros((num_node, num_node)) + np.inf\n","    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n","    arrive_mat = (np.stack(transfer_mat) > 0)\n","    for d in range(max_hop, -1, -1):\n","        hop_dis[arrive_mat[d]] = d\n","    return hop_dis\n","\n","\n","def get_adjacency(hop_dis, center, num_node, max_hop, dilation):\n","    valid_hop = range(0, max_hop + 1, dilation)\n","    adjacency = np.zeros((num_node, num_node))\n","    for hop in valid_hop:\n","        adjacency[hop_dis == hop] = 1\n","    normalize_adjacency = adjacency\n","    A = []\n","    for hop in valid_hop:\n","        a_root = np.zeros((num_node, num_node))\n","        a_close = np.zeros((num_node, num_node))\n","        a_further = np.zeros((num_node, num_node))\n","        for i in range(num_node):\n","            for j in range(num_node):\n","                if hop_dis[j, i] == hop:\n","                    if hop_dis[j, center] == hop_dis[\n","                        i, center]:\n","                        a_root[j, i] = normalize_adjacency[j, i]\n","                    elif hop_dis[j,\n","                                 center] > hop_dis[i,\n","                                                   center]:\n","                        a_close[j, i] = normalize_adjacency[j, i]\n","                    else:\n","                        a_further[j, i] = normalize_adjacency[j, i]\n","        if hop == 0:\n","            A.append(a_root)\n","        else:\n","            A.append(a_root + a_close)\n","            A.append(a_further)\n","    A = np.stack(A)\n","    return (A)\n","\n","\n","layout = 'ntu-rgb+d',\n","strategy = 'spatial'\n","max_hop = 1\n","dilation = 1\n","num_node = 25\n","edge, center = get_edge()\n","hop_dis = get_hop_distance(num_node, edge, max_hop=max_hop)\n","A = get_adjacency(hop_dis, center, num_node, max_hop, dilation)\n","A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n","\n","\n","#######################################################################\n","\n","class ConvTemporalGraphical(nn.Module):\n","    r\"\"\"The basic module for applying a graph convolution.\n","    Args:\n","        in_channels (int): Number of channels in the input sequence data\n","        out_channels (int): Number of channels produced by the convolution\n","        kernel_size (int): Size of the graph convolving kernel\n","        t_kernel_size (int): Size of the temporal convolving kernel\n","        t_stride (int, optional): Stride of the temporal convolution. Default: 1\n","        t_padding (int, optional): Temporal zero-padding added to both sides of\n","            the input. Default: 0\n","        t_dilation (int, optional): Spacing between temporal kernel elements.\n","            Default: 1\n","        bias (bool, optional): If ``True``, adds a learnable bias to the output.\n","            Default: ``True``\n","    Shape:\n","        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n","        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n","        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n","        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n","        where\n","            :math:`N` is a batch size,\n","            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n","            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n","            :math:`V` is the number of graph nodes.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size,\n","                 t_kernel_size=1,\n","                 t_stride=1,\n","                 t_padding=0,\n","                 t_dilation=1,\n","                 bias=True):\n","        super().__init__()\n","\n","        self.kernel_size = kernel_size\n","        self.conv = nn.Conv2d(\n","            in_channels,\n","            out_channels * kernel_size,\n","            kernel_size=(t_kernel_size, 1),\n","            padding=(t_padding, 0),\n","            stride=(t_stride, 1),\n","            dilation=(t_dilation, 1),\n","            bias=bias)\n","\n","    def forward(self, x, A):\n","        assert A.size(0) == self.kernel_size\n","\n","        x = self.conv(x)\n","\n","        n, kc, t, v = x.size()\n","        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n","        x = torch.einsum('nkctv,kvw->nctw', (x, A))\n","\n","        return x.contiguous(), A\n","\n","\n","######################################################################\n","\n","class st_gcn(nn.Module):\n","    r\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n","    Args:\n","        in_channels (int): Number of channels in the input sequence data\n","        out_channels (int): Number of channels produced by the convolution\n","        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n","        stride (int, optional): Stride of the temporal convolution. Default: 1\n","        dropout (int, optional): Dropout rate of the final output. Default: 0\n","        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n","    Shape:\n","        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n","        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n","        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n","        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n","        where\n","            :math:`N` is a batch size,\n","            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n","            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n","            :math:`V` is the number of graph nodes.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size,\n","                 stride=1,\n","                 dropout=0,\n","                 residual=True):\n","        super().__init__()\n","\n","        assert len(kernel_size) == 2\n","        assert kernel_size[0] % 2 == 1\n","        padding = ((kernel_size[0] - 1) // 2, 0)\n","\n","        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n","                                         kernel_size[1])\n","        '''\n","        self.tcn = nn.Sequential(\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels,\n","                (kernel_size[0], 1),\n","                (stride, 1),\n","                padding,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.Dropout(dropout, inplace=True),\n","        )'''\n","        self.tcn = nn.Sequential(\n","            nn.GroupNorm(8, out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels,\n","                (kernel_size[0], 1),\n","                (stride, 1),\n","                padding,\n","            ),\n","            nn.GroupNorm(8, out_channels),\n","            nn.Dropout(dropout, inplace=True),\n","        )\n","\n","        if not residual:\n","            self.residual = lambda x: 0\n","\n","        elif (in_channels == out_channels) and (stride == 1):\n","            self.residual = lambda x: x\n","\n","        else:\n","            self.residual = nn.Sequential(\n","                nn.Conv2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size=1,\n","                    stride=(stride, 1)),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, A):\n","\n","        res = self.residual(x)\n","        x, A = self.gcn(x, A)\n","        x = self.tcn(x) + res\n","\n","        return self.relu(x), A\n","\n","\n","######################################################################\n","\n","class Model(nn.Module):\n","    r\"\"\"Spatial temporal graph convolutional networks.\n","    Args:\n","        in_channels (int): Number of channels in the input data\n","        num_class (int): Number of classes for the classification task\n","        graph_args (dict): The arguments for building the graph\n","        edge_importance_weighting (bool): If ``True``, adds a learnable\n","            importance weighting to the edges of the graph\n","        **kwargs (optional): Other parameters for graph convolution units\n","    Shape:\n","        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n","        - Output: :math:`(N, num_class)` where\n","            :math:`N` is a batch size,\n","            :math:`T_{in}` is a length of input sequence,\n","            :math:`V_{in}` is the number of graph nodes,\n","            :math:`M_{in}` is the number of instance in a frame.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, num_class, A,\n","                 edge_importance_weighting, dropout):\n","        super().__init__()\n","\n","        self.register_buffer('A', A)\n","\n","        # build networks\n","        spatial_kernel_size = A.size(0)\n","        temporal_kernel_size = 9\n","        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n","        self.data_bn = nn.BatchNorm1d(in_channels * A.size(1))\n","\n","        # kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n","        channels = [64, 64, 64, 128, 128, 256]\n","\n","        self.st_gcn_networks = nn.ModuleList((\n","            st_gcn(in_channels, channels[0], kernel_size, 1, dropout=0.1, residual=False),\n","            st_gcn(channels[0], channels[1], kernel_size, 1, dropout=0.2),\n","            st_gcn(channels[1], channels[2], kernel_size, 1, dropout=0.3),\n","            st_gcn(channels[2], channels[3], kernel_size, 2, dropout=0.3),\n","            st_gcn(channels[3], channels[4], kernel_size, 2, dropout=0.3),\n","            st_gcn(channels[4], channels[5], kernel_size, 2, dropout=0.3),\n","        ))\n","\n","        # initialize parameters for edge importance weighting\n","        if edge_importance_weighting:\n","            self.edge_importance = nn.ParameterList([\n","                nn.Parameter(torch.ones(self.A.size()))\n","                for i in self.st_gcn_networks\n","            ])\n","        else:\n","            self.edge_importance = [1] * len(self.st_gcn_networks)\n","\n","        last_channels = channels[-1]\n","        self.fcn = nn.Conv2d(last_channels, num_class, kernel_size=1)\n","\n","    def forward(self, x):\n","\n","        # data normalization\n","        N, C, T, V, M = x.size()\n","        x = x.permute(0, 4, 3, 1, 2).contiguous()\n","        x = x.view(N * M, V * C, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, M, V, C, T)\n","        x = x.permute(0, 1, 3, 4, 2).contiguous()\n","        x = x.view(N * M, C, T, V)\n","\n","        # forwad\n","        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n","            x, _ = gcn(x, self.A * importance)\n","\n","        # global pooling\n","        x = F.avg_pool2d(x, x.size()[2:])\n","        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n","\n","        # prediction\n","        x = self.fcn(x)\n","        x = x.view(x.size(0), -1)\n","\n","        return x\n","\n","    def extract_feature(self, x):\n","\n","        # data normalization\n","        N, C, T, V, M = x.size()\n","        x = x.permute(0, 4, 3, 1, 2).contiguous()\n","        x = x.view(N * M, V * C, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, M, V, C, T)\n","        x = x.permute(0, 1, 3, 4, 2).contiguous()\n","        x = x.view(N * M, C, T, V)\n","\n","        # forwad\n","        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n","            x, _ = gcn(x, self.A * importance)\n","\n","        _, c, t, v = x.size()\n","        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n","\n","        # prediction\n","        x = self.fcn(x)\n","        output = x.view(N, M, -1, t, v).permute(0, 2, 3, 4, 1)\n","\n","        return output, feature\n","\n","\n","#########################################################################\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv1d') != -1 or classname.find('Conv2d') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias.data, 0)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","\n","#########################################################################\n","\n","def accuracy(output, labels):\n","    preds = output.argmax(dim=1)\n","    correct = (preds == labels).float().sum()\n","    return correct / labels.size(0)\n","\n","\n","##########################################################################\n","\n","\n","model = Model(in_channels=3, num_class=num_class, A=A,\n","              edge_importance_weighting=True, dropout=dropout).to(device)\n","model.apply(weights_init)\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n","        super().__init__()\n","        # alpha: class-wise weight (tensor of shape [num_class]) or scalar\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, logits, targets):\n","        # CE per-sample\n","        ce_loss = F.cross_entropy(logits, targets, reduction=\"none\", weight=self.alpha)\n","        # p_t = exp(-CE)\n","        pt = torch.exp(-ce_loss)\n","        # focal factor\n","        loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == \"mean\":\n","            return loss.mean()\n","        elif self.reduction == \"sum\":\n","            return loss.sum()\n","        return loss\n","\n","\n","#criterion = nn.CrossEntropyLoss()\n","criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n","\n","\n","optimizer = optim.AdamW(\n","    model.parameters(),\n","    lr=1e-3,\n","    weight_decay=1e-4\n",")\n","\n","scaler = GradScaler(device if device.type == \"cuda\" else \"cpu\")\n","\n","'''\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='max', factor=0.1, patience=5\n",")\n","'''\n","steps_per_epoch = len(train_loader)\n","scheduler = OneCycleLR(\n","    optimizer,\n","    max_lr=3e-3,\n","    total_steps=num_epoch * steps_per_epoch,\n","    pct_start=0.2,\n","    anneal_strategy='cos', # cosine annealing\n","    div_factor=25.0, # initial_lr = max_lr / div_factor\n","    final_div_factor=1e4, # min_lr = initial_lr / final_div_factor\n",")\n","\n","if resume_training:\n","    print(f\"==> Resuming training from epoch {resume_epoch} using saved state...\")\n","    checkpoint = torch.load(state_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","    resume_epoch = checkpoint['epoch']\n","\n","#########################################################\n","# train / val / test 루프\n","\n","def train_one_epoch(epoch, history_train):\n","    model.train()\n","    loss_avg = 0.0\n","    acc_avg = 0.0\n","    count = 0\n","\n","    accumulation_steps = 4\n","\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    for step, (features_batch, labels_batch) in enumerate(train_loader):\n","        features_batch = features_batch.to(device, non_blocking=True)\n","        labels_batch = labels_batch.to(device, non_blocking=True)\n","\n","        with autocast(\"cuda\" if device.type==\"cuda\" else \"cpu\"):\n","            output = model(features_batch)\n","            loss_train = criterion(output, labels_batch)\n","            loss_train = loss_train / accumulation_steps\n","\n","        # gradient 누적\n","        scaler.scale(loss_train).backward()\n","\n","        # accumulation_steps마다 optimizer step\n","        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step() # ✅ OneCycleLR은 매 step 마다 호출해야 함\n","            optimizer.zero_grad(set_to_none=True)\n","\n","\n","        loss_avg += loss_train.item() * accumulation_steps\n","        acc_avg += accuracy(output, labels_batch).item()\n","        count += 1\n","\n","    loss_avg /= count\n","    acc_avg /= count\n","    lr_current = optimizer.param_groups[0]['lr']\n","\n","    print(f\"[Train] Epoch {epoch:03d} | Loss: {loss_avg:.4f} | Acc: {acc_avg:.4f} | LR: {lr_current:.5f}\")\n","\n","    new_row = pd.DataFrame([{\n","        'epoch': epoch,\n","        'loss': loss_avg,\n","        'acc': acc_avg\n","    }])\n","    history_train = pd.concat([history_train, new_row], ignore_index=True)\n","    history_train.to_csv(os.path.join(resultsFolder, 'train_' + resultsFile), index=False)\n","    return history_train\n","\n","def evaluate(epoch, history, loader, split_name=\"Val\"):\n","    model.eval()\n","    loss_avg = 0.0\n","    acc_avg = 0.0\n","    count = 0\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for features_batch, labels_batch in loader:\n","            features_batch = features_batch.to(device, non_blocking=True)\n","            labels_batch = labels_batch.to(device, non_blocking=True)\n","\n","            with autocast(\"cuda\" if device.type==\"cuda\" else \"cpu\"):\n","                output = model(features_batch)\n","                loss_val = criterion(output, labels_batch)\n","\n","            loss_avg += loss_val.item()\n","            acc_avg += accuracy(output, labels_batch).item()\n","            count += 1\n","            preds = output.argmax(dim=1)\n","            all_preds.append(preds.cpu())\n","            all_labels.append(labels_batch.cpu())\n","\n","\n","    loss_avg /= count\n","    acc_avg /= count\n","\n","    all_preds = torch.cat(all_preds).numpy()\n","    all_labels = torch.cat(all_labels).numpy()\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n","\n","    print(f\"[{split_name:4}] Epoch {epoch:03d} | Loss: {loss_avg:.4f} | \"\n","          f\"Acc: {acc_avg:.4f} | MacroF1: {macro_f1:.4f}\")\n","\n","    new_row = pd.DataFrame([{\n","        'epoch': epoch,\n","        'loss': loss_avg,\n","        'acc': acc_avg\n","    }])\n","    history = pd.concat([history, new_row], ignore_index=True)\n","\n","    filename_prefix = split_name.lower()  # 'val', 'test'\n","    history.to_csv(os.path.join(resultsFolder, f'{filename_prefix}_' + resultsFile), index=False)\n","    return history, acc_avg, macro_f1\n","\n","\n","#####################################################################\n","# 클래스별 성능 함수\n","def evaluate_per_class(model, loader, split_name=\"Test\"):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for features_batch, labels_batch in loader:\n","            features_batch = features_batch.to(device, non_blocking=True)\n","            labels_batch = labels_batch.to(device, non_blocking=True)\n","\n","            # AMP 그대로 사용\n","            with autocast(\"cuda\" if device.type==\"cuda\" else \"cpu\"):\n","                output = model(features_batch)\n","\n","            preds = output.argmax(dim=1)\n","            all_preds.append(preds.cpu())\n","            all_labels.append(labels_batch.cpu())\n","\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","\n","    # confusion matrix\n","    cm = confusion_matrix(\n","        all_labels.numpy(),\n","        all_preds.numpy(),\n","        labels=list(range(num_class))  # 0 ~ num_class-1\n","    )\n","    print(f\"\\n[{split_name}] Confusion Matrix (rows=true, cols=pred):\\n{cm}\")\n","\n","    # 클래스별 정밀도/재현율/F1, accuracy\n","    print(f\"\\n[{split_name}] Classification Report:\")\n","    print(classification_report(\n","        all_labels.numpy(),\n","        all_preds.numpy(),\n","        labels=list(range(num_class)),\n","        digits=4\n","    ))\n","\n","######################################################################\n","\n","# 메인 학습: train + val, best model 저장 후 마지막에 test\n","\n","if resume_training:\n","    history_train = pd.read_csv(os.path.join(resultsFolder, 'train_' + resultsFile))\n","    history_val   = pd.read_csv(os.path.join(resultsFolder, 'val_' + resultsFile))\n","    history_test  = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","else:\n","    history_train = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","    history_val   = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","    history_test  = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","\n","\n","best_val_f1 = 0.0\n","\n","# === Early Stopping 설정 ===\n","early_stop_patience = 10\n","no_improve_count = 0\n","\n","start_epoch = resume_epoch + 1 if resume_training else 1\n","\n","for epoch in tqdm(range(start_epoch, num_epoch + 1)):\n","    history_train = train_one_epoch(epoch, history_train)\n","    history_val, val_acc, val_macro_f1 = evaluate(epoch, history_val, val_loader, split_name=\"Val\")\n","\n","    '''\n","    prev_lr = optimizer.param_groups[0]['lr']\n","    scheduler.step(val_macro_f1)\n","    new_lr = optimizer.param_groups[0]['lr']\n","\n","    if new_lr != prev_lr:\n","        print(f\"LR reduced: {prev_lr:.6f} -> {new_lr:.6f} (based on Val MacroF1={val_macro_f1:.4f})\")\n","    '''\n","    # best model 저장\n","    if val_macro_f1 > best_val_f1:\n","        best_val_f1 = val_macro_f1\n","        no_improve_count = 0\n","        torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scaler_state_dict': scaler.state_dict(),\n","        'epoch': epoch\n","    }, state_path)\n","        print(f\"==> Best model updated at epoch {epoch} (Val MacroF1={best_val_f1:.4f})\")\n","    else:\n","        no_improve_count += 1\n","        print(f\"No improvement for {no_improve_count} epoch(s)\")\n","\n","        if no_improve_count >= early_stop_patience:\n","            print(f\"\\nEarly stopping at epoch {epoch} \"\n","                  f\"(no improvement for {early_stop_patience} consecutive epochs)\")\n","            break\n","# 최종 Test: best validation 모델 기준\n","print(\"\\n==> Testing with best validation model...\")\n","best_model = Model(in_channels=3, num_class=num_class, A=A,\n","                   edge_importance_weighting=True, dropout=dropout).to(device)\n","checkpoint = torch.load(state_path, map_location=device)\n","best_model.load_state_dict(checkpoint['model_state_dict'])\n","\n","model = best_model\n","history_test, test_acc, test_macro_f1 = evaluate(\n","    0, history_test, test_loader, split_name=\"Test\"\n",")\n","print(f\"\\nFinal Test Accuracy: {test_acc:.4f} | Final Test MacroF1: {test_macro_f1:.4f}\")\n","evaluate_per_class(model, test_loader, split_name=\"Test\")"]}]}