{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1ApPNo_jKnU_DT4WxMuRyBcSjJcqNb_Jw","authorship_tag":"ABX9TyOyfMzJsJeYnY9NDe0fTa4h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","resultsFolder = \"/content/drive/MyDrive/Colab_Notebooks/Projects/3-2/CV/quick-action-recognition/\"\n","import os\n","os.makedirs(resultsFolder, exist_ok=True)"],"metadata":{"id":"oZhh_Exxinjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NniNWpwriM4I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.optim as optim\n","import pickle\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import WeightedRandomSampler\n","import pandas as pd\n","\n","\n","\n","#########################################################\n","torch.cuda.empty_cache()\n","\n","batch_size = 4\n","num_epoch = 80\n","num_class = 12\n","base_lr = 0.1\n","lr = base_lr\n","dropout = 0.5\n","step = [10, 50]\n","weight_decay = 0.0001\n","experiment = \"stgcn_small_ntu_12\"\n","resultsFile = experiment + \".csv\"\n","state_path = resultsFolder + experiment + \".pth\"\n","path = \"/content/drive/MyDrive/Colab_Notebooks/Projects/3-2/CV/quick-action-recognition/data/NTU-RGB-D/x-view/\"\n","\n","features_train = torch.FloatTensor(np.load(path + \"small_train_data.npy\"))\n","features_test  = torch.FloatTensor(np.load(path + \"small_val_data.npy\"))\n","\n","labels_train = pickle.load(open(path + \"small_train_label.pkl\", \"rb\"))[1]\n","labels_test  = pickle.load(open(path + \"small_val_label.pkl\", \"rb\"))[1]\n","\n","\n","#########################################################\n","\n","num_class = len(np.unique(labels_train))  # small 클래스 개수 자동 계산\n","\n","# Convert to int\n","labels_train = np.array(list(map(int, labels_train)))\n","labels_test  = np.array(list(map(int, labels_test)))\n","\n","# Relabel: 실제 라벨을 0 ~ num_class-1 로 다시 매핑\n","actions = np.unique(labels_train)\n","for j, k in enumerate(actions):\n","    index = [i for i, x in enumerate(labels_test) if x == k]\n","    labels_test[index] = j\n","    index = [i for i, x in enumerate(labels_train) if x == k]\n","    labels_train[index] = j\n","\n","#################### Sampler\n","\n","class_sample_count = np.array([len(np.where(labels_train == t)[0]) for t in np.unique(labels_train)])\n","weight = 1. / class_sample_count\n","samples_weight = np.array([weight[int(t)] for t in labels_train])\n","samples_weight = torch.from_numpy(samples_weight)\n","samples_weight = samples_weight.double()\n","sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n","\n","####################### Trainloader and testloader\n","\n","labels_train = torch.LongTensor(labels_train)\n","train_dataset = torch.utils.data.TensorDataset(features_train, labels_train)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, num_workers=0, sampler=sampler,pin_memory =True)\n","del features_train, labels_train\n","labels_test = torch.LongTensor(labels_test)\n","test_dataset = torch.utils.data.TensorDataset(features_test, labels_test)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, num_workers=0, pin_memory =True)\n","del features_test, labels_test\n","\n","def get_edge():\n","    num_node = 25\n","    self_link = [(i, i) for i in range(num_node)]\n","    neighbor_1base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n","                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n","                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n","                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n","                      (22, 23), (23, 8), (24, 25), (25, 12)]\n","    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n","    edge = self_link + neighbor_link\n","    center = 21 - 1\n","    return (edge, center)\n","\n","\n","def get_hop_distance(num_node, edge, max_hop=1):\n","    A = np.zeros((num_node, num_node))\n","    for i, j in edge:\n","        A[j, i] = 1\n","        A[i, j] = 1\n","\n","    hop_dis = np.zeros((num_node, num_node)) + np.inf\n","    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n","    arrive_mat = (np.stack(transfer_mat) > 0)\n","    for d in range(max_hop, -1, -1):\n","        hop_dis[arrive_mat[d]] = d\n","    return hop_dis\n","\n","\n","def get_adjacency(hop_dis, center, num_node, max_hop, dilation):\n","    valid_hop = range(0, max_hop + 1, dilation)\n","    adjacency = np.zeros((num_node, num_node))\n","    for hop in valid_hop:\n","        adjacency[hop_dis == hop] = 1\n","    normalize_adjacency = adjacency\n","    A = []\n","    for hop in valid_hop:\n","        a_root = np.zeros((num_node, num_node))\n","        a_close = np.zeros((num_node, num_node))\n","        a_further = np.zeros((num_node, num_node))\n","        for i in range(num_node):\n","            for j in range(num_node):\n","                if hop_dis[j, i] == hop:\n","                    if hop_dis[j, center] == hop_dis[\n","                        i, center]:\n","                        a_root[j, i] = normalize_adjacency[j, i]\n","                    elif hop_dis[j,\n","                                 center] > hop_dis[i,\n","                                                   center]:\n","                        a_close[j, i] = normalize_adjacency[j, i]\n","                    else:\n","                        a_further[j, i] = normalize_adjacency[j, i]\n","        if hop == 0:\n","            A.append(a_root)\n","        else:\n","            A.append(a_root + a_close)\n","            A.append(a_further)\n","    A = np.stack(A)\n","    return (A)\n","\n","\n","layout = 'ntu-rgb+d',\n","strategy = 'spatial'\n","max_hop = 1\n","dilation = 1\n","num_node = 25\n","edge, center = get_edge()\n","hop_dis = get_hop_distance(num_node, edge, max_hop=max_hop)\n","A = get_adjacency(hop_dis, center, num_node, max_hop, dilation)\n","A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n","\n","\n","#######################################################################\n","\n","class ConvTemporalGraphical(nn.Module):\n","    r\"\"\"The basic module for applying a graph convolution.\n","    Args:\n","        in_channels (int): Number of channels in the input sequence data\n","        out_channels (int): Number of channels produced by the convolution\n","        kernel_size (int): Size of the graph convolving kernel\n","        t_kernel_size (int): Size of the temporal convolving kernel\n","        t_stride (int, optional): Stride of the temporal convolution. Default: 1\n","        t_padding (int, optional): Temporal zero-padding added to both sides of\n","            the input. Default: 0\n","        t_dilation (int, optional): Spacing between temporal kernel elements.\n","            Default: 1\n","        bias (bool, optional): If ``True``, adds a learnable bias to the output.\n","            Default: ``True``\n","    Shape:\n","        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n","        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n","        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n","        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n","        where\n","            :math:`N` is a batch size,\n","            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n","            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n","            :math:`V` is the number of graph nodes.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size,\n","                 t_kernel_size=1,\n","                 t_stride=1,\n","                 t_padding=0,\n","                 t_dilation=1,\n","                 bias=True):\n","        super().__init__()\n","\n","        self.kernel_size = kernel_size\n","        self.conv = nn.Conv2d(\n","            in_channels,\n","            out_channels * kernel_size,\n","            kernel_size=(t_kernel_size, 1),\n","            padding=(t_padding, 0),\n","            stride=(t_stride, 1),\n","            dilation=(t_dilation, 1),\n","            bias=bias)\n","\n","    def forward(self, x, A):\n","        assert A.size(0) == self.kernel_size\n","\n","        x = self.conv(x)\n","\n","        n, kc, t, v = x.size()\n","        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n","        x = torch.einsum('nkctv,kvw->nctw', (x, A))\n","\n","        return x.contiguous(), A\n","\n","\n","######################################################################\n","\n","class st_gcn(nn.Module):\n","    r\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n","    Args:\n","        in_channels (int): Number of channels in the input sequence data\n","        out_channels (int): Number of channels produced by the convolution\n","        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n","        stride (int, optional): Stride of the temporal convolution. Default: 1\n","        dropout (int, optional): Dropout rate of the final output. Default: 0\n","        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n","    Shape:\n","        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n","        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n","        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n","        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n","        where\n","            :math:`N` is a batch size,\n","            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n","            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n","            :math:`V` is the number of graph nodes.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size,\n","                 stride=1,\n","                 dropout=0,\n","                 residual=True):\n","        super().__init__()\n","\n","        assert len(kernel_size) == 2\n","        assert kernel_size[0] % 2 == 1\n","        padding = ((kernel_size[0] - 1) // 2, 0)\n","\n","        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n","                                         kernel_size[1])\n","\n","        self.tcn = nn.Sequential(\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels,\n","                (kernel_size[0], 1),\n","                (stride, 1),\n","                padding,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.Dropout(dropout, inplace=True),\n","        )\n","\n","        if not residual:\n","            self.residual = lambda x: 0\n","\n","        elif (in_channels == out_channels) and (stride == 1):\n","            self.residual = lambda x: x\n","\n","        else:\n","            self.residual = nn.Sequential(\n","                nn.Conv2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size=1,\n","                    stride=(stride, 1)),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, A):\n","\n","        res = self.residual(x)\n","        x, A = self.gcn(x, A)\n","        x = self.tcn(x) + res\n","\n","        return self.relu(x), A\n","\n","\n","######################################################################\n","\n","class Model(nn.Module):\n","    r\"\"\"Spatial temporal graph convolutional networks.\n","    Args:\n","        in_channels (int): Number of channels in the input data\n","        num_class (int): Number of classes for the classification task\n","        graph_args (dict): The arguments for building the graph\n","        edge_importance_weighting (bool): If ``True``, adds a learnable\n","            importance weighting to the edges of the graph\n","        **kwargs (optional): Other parameters for graph convolution units\n","    Shape:\n","        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n","        - Output: :math:`(N, num_class)` where\n","            :math:`N` is a batch size,\n","            :math:`T_{in}` is a length of input sequence,\n","            :math:`V_{in}` is the number of graph nodes,\n","            :math:`M_{in}` is the number of instance in a frame.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, num_class, A,\n","                 edge_importance_weighting, dropout):\n","        super().__init__()\n","\n","        self.register_buffer('A', A)\n","\n","        # build networks\n","        spatial_kernel_size = A.size(0)\n","        temporal_kernel_size = 9\n","        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n","        self.data_bn = nn.BatchNorm1d(in_channels * A.size(1))\n","\n","        # kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n","        self.st_gcn_networks = nn.ModuleList((\n","            st_gcn(in_channels, 64, kernel_size, 1, dropout=dropout, residual=False),\n","            st_gcn(64, 64, kernel_size, 1, dropout=dropout),\n","            st_gcn(64, 64, kernel_size, 1, dropout=dropout),\n","            st_gcn(64, 128, kernel_size, 2, dropout=dropout),\n","            st_gcn(128, 128, kernel_size, 1, dropout=dropout),\n","            st_gcn(128, 256, kernel_size, 2, dropout=dropout),\n","            st_gcn(256, 256, kernel_size, 1, dropout=dropout),\n","        ))\n","\n","        # initialize parameters for edge importance weighting\n","        if edge_importance_weighting:\n","            self.edge_importance = nn.ParameterList([\n","                nn.Parameter(torch.ones(self.A.size()))\n","                for i in self.st_gcn_networks\n","            ])\n","        else:\n","            self.edge_importance = [1] * len(self.st_gcn_networks)\n","\n","        # fcn for prediction\n","        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n","\n","    def forward(self, x):\n","\n","        # data normalization\n","        N, C, T, V, M = x.size()\n","        x = x.permute(0, 4, 3, 1, 2).contiguous()\n","        x = x.view(N * M, V * C, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, M, V, C, T)\n","        x = x.permute(0, 1, 3, 4, 2).contiguous()\n","        x = x.view(N * M, C, T, V)\n","\n","        # forwad\n","        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n","            x, _ = gcn(x, self.A * importance)\n","\n","        # global pooling\n","        x = F.avg_pool2d(x, x.size()[2:])\n","        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n","\n","        # prediction\n","        x = self.fcn(x)\n","        x = x.view(x.size(0), -1)\n","\n","        return x\n","\n","    def extract_feature(self, x):\n","\n","        # data normalization\n","        N, C, T, V, M = x.size()\n","        x = x.permute(0, 4, 3, 1, 2).contiguous()\n","        x = x.view(N * M, V * C, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, M, V, C, T)\n","        x = x.permute(0, 1, 3, 4, 2).contiguous()\n","        x = x.view(N * M, C, T, V)\n","\n","        # forwad\n","        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n","            x, _ = gcn(x, self.A * importance)\n","\n","        _, c, t, v = x.size()\n","        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n","\n","        # prediction\n","        x = self.fcn(x)\n","        output = x.view(N, M, -1, t, v).permute(0, 2, 3, 4, 1)\n","\n","        return output, feature\n","\n","\n","#########################################################################\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv1d') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif classname.find('Conv2d') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","\n","#########################################################################\n","\n","def accuracy(output, labels):\n","    output = F.log_softmax(output, dim=1)\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)\n","\n","\n","##########################################################################\n","\n","\n","def train(epoch, history_train, base_lr, step):\n","    model.train()\n","\n","    lr = base_lr * (0.1 ** np.sum(epoch >= np.array(step)))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    #########################################\n","    acc_avg = 0\n","    loss_avg = 0\n","    count = 0\n","\n","    for i, c in enumerate(train_loader):\n","        features_batch = c[0].cuda()\n","        labels_batch = c[1].cuda()\n","        optimizer.zero_grad()\n","        output = model(features_batch)\n","        loss_train = loss(output, labels_batch)\n","        loss_train.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        loss_avg += float(loss_train.cuda().item())\n","        acc_train = float(accuracy(output, labels_batch).cuda().item())\n","        acc_avg += acc_train\n","        count += 1\n","\n","    loss_avg = loss_avg / count\n","    acc_avg = acc_avg / count\n","\n","    print(f\"[Train] Epoch {epoch:03d} | Loss: {loss_avg:.4f} | Acc: {acc_avg:.4f} | LR: {lr}\")\n","    new_row = pd.DataFrame([{\n","    'epoch': epoch,\n","    'loss': loss_avg,\n","    'acc': acc_avg\n","    }])\n","\n","    history_train = pd.concat([history_train, new_row], ignore_index=True)\n","    history_train.to_csv(resultsFolder + 'train_' + resultsFile, index=False)\n","    return history_train\n","\n","\n","def test(epoch, history_test):\n","    acc_avg = 0\n","    loss_avg = 0\n","    count = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for i, c in enumerate(test_loader):\n","            features_batch = c[0].cuda()\n","            labels_batch = c[1].cuda()\n","            output = model(features_batch)\n","            loss_avg += float(loss(output, labels_batch).cuda().item())\n","            # loss_avg += F.nll_loss(output, labels_batch)\n","            acc_avg += float(accuracy(output, labels_batch).cuda().item())\n","            count += 1\n","    loss_avg = loss_avg / count\n","    acc_avg = acc_avg / count\n","\n","    print(f\"[Test ] Epoch {epoch:03d} | Loss: {loss_avg:.4f} | Acc: {acc_avg:.4f}\")\n","\n","    new_row = pd.DataFrame([{\n","    'epoch': epoch,\n","    'loss': loss_avg,\n","    'acc': acc_avg\n","    }])\n","\n","    history_test = pd.concat([history_test, new_row], ignore_index=True)\n","    history_test.to_csv(resultsFolder + 'test_' + resultsFile, index=False)\n","    return history_test\n","\n","\n","######################################################################\n","\n","model = Model(in_channels=3, num_class=num_class, A=A,\n","              edge_importance_weighting=True, dropout=dropout)\n","\n","seed = 42\n","cuda = torch.cuda.is_available()\n","if cuda:\n","    torch.cuda.manual_seed(seed)\n","    A = A.cuda()\n","    model = model.cuda()\n","\n","model.apply(weights_init)\n","loss = nn.CrossEntropyLoss()\n","\n","# Adam\n","optimizer = optim.Adam(\n","    model.parameters(),\n","    lr=base_lr)\n","optimizer = optim.SGD(\n","    model.parameters(),\n","    lr=base_lr,\n","    momentum=0.9,\n","    nesterov=True,\n","    weight_decay=weight_decay)\n","\n","########################################################\n","\n","history_train = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","history_test = pd.DataFrame({'epoch': [], 'loss': [], 'acc': []})\n","for i in tqdm(range(num_epoch)):\n","    history_train = train(i, history_train, base_lr, step)\n","    history_test = test(i, history_test)\n","    torch.save(model.state_dict(), state_path)"]}]}